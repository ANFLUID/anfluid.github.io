<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>ANFluid: Animate Natural Fluid Photos base on Physics-Aware Simulation and Dual-Flow Texture Learning</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">ANFluid: Animate Natural Fluid Photos base on Physics-Aware Simulation and Dual-Flow Texture Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Xiangcheng Zhai</a><sup>1</sup>,
              </span>
              <span class="author-block">
                <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Yingqi Jie</a><sup>2</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Xueguang Xie</a><sup>4</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Aimin Hao</a><sup>3</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Na Jiang</a><sup>1*</sup>,
              </span>
              <span class="author-block">
                <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Yang Gao</a><sup>3*</sup>
              </span>
                  </div>
              <!-- Affiliations block -->
              <div class="author-affiliations">
                <p><sup>1</sup> Capital Normal University, Beijing, China</p>
                <p><sup>2</sup> Beijing Institute of Technology, Beijing, China</p>
                <p><sup>3</sup> Beihang University, Beijing, China</p>
                <p><sup>4</sup> University of Science and Technology Beijing, Beijing, China</p>
              </div>
                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div> -->

                  <!-- <div class="column has-text-centered"> -->
                    <!-- <div class="publication-links"> -->
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block"> -->
                        <!-- <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank" -->
                        <!-- class="external-link button is-normal is-rounded is-dark"> -->
                        <!-- <span class="icon"> -->
                          <!-- <i class="fas fa-file-pdf"></i> -->
                        <!-- </span> -->
                        <!-- <span>Paper</span> -->
                      <!-- </a> -->
                    <!-- </span> -->

                    <!-- Supplementary PDF link
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span>

                  Github link -->
                  <!-- <span class="link-block"> -->
                    <!-- <a href="https://github.com/YOUR REPO HERE" target="_blank" -->
                    <!-- class="external-link button is-normal is-rounded is-dark"> -->
                    <!-- <span class="icon"> -->
                      <!-- <i class="fab fa-github"></i> -->
                    <!-- </span> -->
                    <!-- <span>Code</span> -->
                  <!-- </a> -->
                <!-- </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video -->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%"> -->
        <!-- Your video here -->
        <!-- <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Generating photorealistic animations from a single still photo represents a significant advancement in multimedia editing and artistic creation. While existing AIGC methods have reached milestone successes, they often struggle with maintaining consistency with real-world physical laws, particularly in fluid dynamics. To address this issue, this paper introduces ANFluid, a physics solver and datadriven coupled framework that combines physics-aware simulation (PAS) and dual-flow texture learning (DFTL) to animate natural fluid photos effectively. The PAS component of ANFluid ensuresthat motion guides adhere to physical laws, and can be automatically tailored with specific numerical solver to meet the diversities of different fluid scenes. Concurrently, DFTL focuses on enhancing texture prediction. It employs bidirectional self-supervised optical flow estimation and multi-scale wrapping to strengthen dynamic relationships and elevate the overall animation quality. Notably, despite being built on a transformer architecture, the innovative encoder-decoder design in DFTL does not increase the parameter count but rather enhances inference efficiency. Extensive quantitative experiments have shown that our ANFluid surpasses most current methods on the Holynski and CLAW datasets. User studies further confirm that animations produced by ANFluid maintain better physical and content consistency with the real world and the original input, respectively. Moreover, ANFluid supports interactive editing during the simulation process, enriching the animation content and broadening its application potential.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">

       <!-- First item -->
       <div class="item has-text-centered">
        <!-- Image centered -->
        <img src="static/images/outline.jpg" alt="MY ALT TEXT" style="display: block; margin: 0 auto;"/>
        <!-- Text centered, justified and bold -->
        <h2 class="subtitle has-text-justified">
          <strong>Overview of the proposed ANFluid.</strong> ANFluid is composed of Data Preprocessing (DP), PAS and DFTL. DP is responsible for generating a depth map and initial motion field from input images and prompts. In init Motion, different colors represent the direction of pixel movement, while the color saturation indicates the speed of motion. PAS estimates the motion field, and DFTL exploits a multi-scale wrapping image texture feature learning network (MWT) and a bidirectional self-supervised optical flow estimation network (BSF) to achieves texture prediction. MWT creates fluid animations iteratively by extracting and wrapping image features based on estimated motion field. BSF enhances texture feature association by providing dual-flow constrained motion for MWT during training.
        </h2>
      </div>

      <!-- Second item -->
      <div class="item has-text-centered">
        <!-- Image centered -->
        <img src="static/images/BSF.jpg" alt="MY ALT TEXT" style="display: block; margin: 0 auto;"/>
        <!-- Text centered, justified and bold -->
        <h2 class="subtitle has-text-justified">
          <strong>Bidirectional self-supervised optical flow estimation.</strong> In training, bidirectional video sequences are input into the optical flow estimation network to generate optical flow for both forward and backward sequences, enforcing consistency constraints between forward and backward flow.
        </h2>
      </div>

      <!-- <div class="item"> -->
        <!-- Your image here -->
        <!-- <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Third image description.
       </h2>
     </div>
     <div class="item"> -->
      <!-- Your image here -->
      <!-- <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Fourth image description.
      </h2>
    </div> -->
  </div>
</div>
</div>
</section>
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container"> -->
      <!-- Paper video. -->
      <!-- <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video"> -->
            <!-- Youtube embed code here -->
            <!-- <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video Section -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Results</h2>
      <div class="video-container">
        <video poster="" id="video1" autoplay controls muted loop height="100%">
          <!-- Your video file here -->
          <source src="static/videos/result.mp4" type="video/mp4">
        </video>
      </div>
    </div>
  </div>
</section>
<!-- End video section -->







<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Here</code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
